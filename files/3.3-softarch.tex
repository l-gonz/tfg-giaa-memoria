\section{Software architecture}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/software-arch.jpg}
  \caption{Structure of the Dronecontrol application and its interactions with the necessary additional software for running in a simulation (green) or in an actual vehicle (blue).}\label{fig:architecture}
\end{figure}


Figure \ref{fig:architecture} shows the main modules of the designed software and how it interacts with the external libraries.
The application consists of three basic parts: 
the pilot module, in charge of sending instructions to the flight controller and receiving back information on position and state through the \texttt{mavsdk} library, 
a video source module that handles the retrieval of images from different sources and the necessary processing for image analysis,
and a control module that directs the interaction between the other two to transform the pixel information first into position points through the \texttt{mediapipe} library and then into instructions for the pilot.
The upper part of the diagram in figure \ref{fig:architecture} shows the flow of information between the Dronecontrol application and the external systems, with green lines representing its path in a simulated workflow and blue ones indicating the alternative path for the information in a system with actual quadcopter hardware.
Purple arrows show the input/output of each module within the developed application and how they interconnect with each other.
Other smaller utilities have also been developed to help test how the systems interact with each other and calibrate different parts of the control behavior.
These are described in sections \ref{subsec:cam-tool} and \ref{subsec:pid-tools}.
A user manual with all the options available in the application can be found in Appendix \ref{app:cli}.

\subsection{Pilot module}
\label{subsec:pilot-module}
The purpose of the pilot module is to provide access to the rest of the application to send and receive messages from the PX4 controller through the MavSDK library.
This library provides a simple asynchronous API for managing one or more vehicles, providing programmatic access to vehicle information and telemetry, and control over missions, movement and other operations.
MavSDK utilizes the python standard library \emph{asyncio} to be able to run coroutines in parallel while waiting for the messages provided through the \emph{MAVLink} communication.
Therefore all calls to the library have to be written as async functions that await the result of one or more polls to the flight stack.

\texttt{asyncio} provides support for writing concurrent code using the \texttt{async/await} syntax.
It is used as a foundation for multiple Python asynchronous frameworks that provide high-performance network and web-servers, database connection libraries, distributed task queues, etc; 
and provides a set of high-level APIs to run Python coroutines concurrently and have full control over their execution.
The pilot module integrates \texttt{mavsdk} and \texttt{asyncio} and provides a queue for the control module to send actions to be executed in the vehicle one after another.

Listing~\ref{lst:pilot.connect} shows how to establish a connection to a PX4 vehicle through its physical (serial) or virtual (UDP) address and poll for internal information from the flight controller to decide when the system is ready to receive instructions.
The mavsdk library exposes telemetry and other state information through asynchronous generators, which are defined in python as a convenient way to make asynchronous data producers and accessed with the \mintinline{python}{async for} syntax.


\begin{listing}[h!]
    \caption{Example of how the communication to the flight stack is established through asyncio and the mavsdk library}{}
    \label{lst:pilot.connect}
    \begin{minted}[breaklines, fontsize=\footnotesize, baselinestretch=1]{python}
async def connect(self):
    """Connect to mavsdk server.
       Raises a TimeoutError if it is not possible to establish connection.
    """
    
    if self.serial:
        address = f"serial://{self.serial}"
    else:
        address = f"udp://{self.ip if self.ip else ''}:{self.port}"
    self.log.info("Waiting for drone to connect on address " + address)
    await asyncio.wait_for(self.mav.connect(system_address=address), timeout=self.TIMEOUT)

    async for state in self.mav.core.connection_state():
        if state.is_connected:
            break

    # Wait for drone to have a global position estimate
    async for health in self.mav.telemetry.health():
        if health.is_global_position_ok:
            break
        
    self.log.info("System ready")
    self.is_ready = True
    \end{minted}
\end{listing}

Many of basic operations that can be executed in the flight controller are implemented in the pilot module with error handling and safety checks, like takeoff, landing, return home, or manipulating the vehicle flying velocity directly by providing speeds in body coordinates.
These actions can be executed directly or added to a queue that runs in a loop executing them in the order they are added, waiting until the previous action has finished and the vehicle is in the desired state before starting the next.
The loop that runs this queue can be seen in Listing~\ref{lst:pilot.queue}.
There is a maximum time of 10 seconds that each action can use to run
The loop stops when the asynchronous task it runs on is canceled with \mintinline{python}{task.cancel()}, which raises a \mintinline{python}{CancelledError} exception in the parallel execution.

\begin{listing}[h!]
    \caption{Loop where the action queue runs on the pilot module. Each action is awaited until it finishes or the timeout time runs out.}{}
    \label{lst:pilot.queue}
    \begin{minted}[breaklines, fontsize=\footnotesize, baselinestretch=1]{python}
async def run_queue(self):
    """
    Run the queue loop.
    
    Queued actions will be awaited one at a time
    until they are finished.
    The loop will sleep if the queue is empty.
    """
    try:
        while True:
            if len(self.actions) > 0:
                action = self.actions.pop(0)
                self.log.info("Execute action: %s", action.func.__name__)
                try:
                    await asyncio.wait_for(action.func(self, **action.args), timeout=10)
                except asyncio.exceptions.TimeoutError:
                    self.log.warning(f"Time out waiting for {action.func.__name__}")
            else:
                await asyncio.sleep(self.WAIT_TIME)
    except asyncio.exceptions.CancelledError:
        self.log.warning("System stop")
    \end{minted}
\end{listing}

\todo[inline]{Polish: consider removing code or exchanging for diagrams}

\subsection{Video source module}
\label{subsec:viz-source-module}

The objective of the video source module is to provide a collection of classes to retrieve images from different sources,
in a way that they can be exchanged for one another without affecting the rest of the application to facilitate testing and be adaptable to running in different environments.
There are three classes of video sources implemented: file, simulator, and camera, which inherit from the same \mintinline{python}{VideoSource} base class as shown in figure \ref{fig:video-source-inheritance}.

\begin{figure}
  \centering
  \includegraphics[width=8cm, keepaspectratio]{img/uml-video-source.jpg}
  \caption{Diagram of inheritance on the video source classes available to retrieve image data.}
  \label{fig:video-source-inheritance}
\end{figure}

The \mintinline{python}{FileSource} class is able to open a video file stored in the companion computer and provides images taken frame by frame from it until the video ends.
This allows the program to replay the image detection algorithms on videos previously captured by the camera tool exposed in section~\ref{subsec:test-tools}.
The \mintinline{python}{CameraSource} class can access a physical camera attached to the computer running the application via USB and provide the frames captured in real-time.
Both the file and camera sources employ OpenCV's video capture utilities to take care of the file handling and the camera driver management capabilities respectively.

The simulator source uses AirSim's Python library to communicate with the simulator and retrieve images from a simulated camera attached to the 3D model of the vehicle in Unreal Engine.
It connects automatically through localhost, but it can also be provided with an IP to establish connection to the simulator running on a different computer on the local network, for example when the program runs inside WSL and the simulator runs on its host computer.

\subsection{Vision control module}
\label{subsec:control-module}
The control module contains the main logic of the application and is in charge of converting the raw images obtained from the video source into commands for the pilot module.
Two different types of control have been implemented.
The first one is a proof-of-concept control solution, described in section \ref{sec:hands}, that runs in offboard mode (see section \ref{subsec:offboard}) and translates some predefined hand gestures into simple commands for the aerial vehicle, of which the main purpose is to be able to test the interaction between all the components of the system in a more easily controlled environment, since it uses the simpler configuration of situating the computer with the controller outside of the vehicle.
The second control system consists of a follow solution more applicable to real-life scenarios where the control and the camera is onboard the vehicle and the presence of a person is detected in the images obtained from the perspective of the drone in order to be able to give the flight controller velocity commands to follow said person and maintain it centered in its view.

The process followed in both solutions consists roughly of the same two parts.
First the image is sent to the computer vision and machine learning third-party library that extracts the required features from the image in the form of 2D coordinates.
Afterwards a series of calculations depending on the particular solution are applied to these coordinates to decide which commands are sent to the pilot module.
The third-party library used for computer vision in the program is the mediapipe library described in Section~\ref{subsec:mediapipe}, which offers cross-platform, customizable ML solutions for live and streaming media, specifically its hand and pose detection solutions.

To engage the module in direct control of the vehicle's velocity it is necessary to use a special flight mode defined by PX4 for this purpose, called Offboard Mode \footnote{\url{https://docs.px4.io/main/en/flight_modes/offboard.html\#offboard-mode}} (not to be confused with the offboard configuration described in section~\ref{subsec:offboard}).
Offboard mode is primarily used for controlling vehicle movement and attitude, and supports only a very limited set of MAVLink messages. This mode requires position or pose/attitude information to be available to the flight controller, e.g. GPS.
In it, the vehicle obeys a position, velocity or attitude setpoint provided over MAVLink by a MAVLink API (i.e. MAVSDK) running on a companion computer and usually connected via serial cable or wifi.
A stream of setpoint commands must be received by the vehicle at a rate higher than 2Hz prior to engaging the mode and in order to remain in it.
If the message rate falls below 2Hz or the connection is lost the vehicle will stop and, after a timeout, the vehicle will attempt to land or perform some other failsafe action according to the parameters configured.
In order to hold position while in this mode, the vehicle must receive a stream of setpoints for the current position.

Sections~\ref{sec:hands} and \ref{sec:follow} offer a more complete explanation of the control module used by the two different solutions developed.

\subsubsection{Camera-testing tool}
\label{subsec:cam-tool}

Several additional utilities have been added to the dronecontrol program to facilitate the development and testing process of the two main control solutions.
The first tool is accessed through the command \mintinline{bash}{dronecontrol tools test-camera} and can be used for testing the connection between the computer, the flight stack and the camera without using any self-guided control mechanism, as well as evaluating the performance of the MediaPipe hand and pose machine learning solution on real time images.
It is also possible to take images and record video from the live camera feed for later analysis.
Through the command-line options, the test tool can be configured to use any of the three available video sources: camera, simulator or a video file recorded by using the tool itself,
to connect to an optional hardware or simulated PX4 flight controller by specifying a connection string or IP, respectively,
to run either in a host computer or a WSL subsystem,
and to process incoming images with the hand or pose recognition software.
While the tool is running, the keyboard can be used to send basic commands to a connected vehicle like takeoff, landing or moving along any of its three axis.
A full breakdown of all the tool's options can be found in Appendix \ref{app:cli}.




\section{Proof of concept: hand-gesture solution}
\label{sec:hands}
The main purpose of this solution is to test that the flow of the application works as expected, both in simulation and in real flight, and that all the systems are capable of establishing the required connections with each other.
For that reason it is designed to be able to be run in real flight with the minimal setup of a built drone with its default components and any computer with a camera.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/hand_landmarks.png}
  \caption{Landmarks extracted from detected hands by the MediaPipe hand solution.}
  \source{Adapted from \citetitle{mp-hands} \cite{mp-hands}.}
  \label{fig:hand-landmarks}
\end{figure}

This control module runs on a loop that continuously polls for a new frame from the chosen video source and feeds it to the hand detection functionality from MediaPipe \cite{mp-hands-paper}.
If a hand is detected in the images, 2D coordinates are extracted according to the map shown in figure~\ref{fig:hand-landmarks}.
These landmarks are then converted into different discrete gestures like open palm, closed fist or an specific finger pointing different directions.
When a new gesture is detected, the command assigned to it is queued to the pilot module and executed as soon as the previous commands end their execution.

The conversion between landmarks and gestures is performed by drawing vectors from the base of each of the fingers to their tips as well as from the base of the hand (wrist feature) to the base of the fingers and using the dot product vector operation to calculate the relative angles between each finger and the base of the hand, as shown in figure~\ref{fig:vector-calcs}.
By comparing the calculated angles to a threshold, it is possible to detect whether each individual finger is extended or folded, as well as the general direction it is pointing towards.
The open hand gesture, for example, can then be defined as all five fingers extended, that is, all five vectors defined by the fingers sharing the same approximate angle with the vector from the base of the hand to that finger.

\begin{figure}
  \centering
  \includegraphics[width=8cm, keepaspectratio]{img/placeholder.png}
  \caption{Vectors extracted from the detected features to calculate the relative positions of the individual fingers}
  \label{fig:vector-calcs}
\end{figure}
\todo[inline]{Figure \ref{fig:vector-calcs}: hand with arrow vectors between pertinent features and angles origins shown}

The full list of gestures detected by the program by calculating these angles is shown in figure \ref{fig:hand-gestures} and is as follows:
\begin{itemize}
    \item No hand: happens when no landmarks are able to be extracted from the image. As a safety feature, in this case, the vehicle stops whichever previous commands it had in its queue and goes into hold flight mode, where it just hovers in the air maintaining its position.
    \item Open hand: it is detected when all five fingers are extended, as if gesturing stop, and makes the drone land at its current position.
    \item Fist: it is detected when all five fingers are folded and makes the drone arm and takeoff. If the drone is already in the air nothing happens.
    \item Index finger pointing up: it is detected when only the index finger is extended and it is pointing roughly towards the top of the image ($\pm 30$ degrees) and makes the drone go into offboard mode, where it is possible to receive direct velocity commands.
    \item Index finger pointing to the right: same as above but pointing to the right of the image and makes the drone roll towards its right side at a speed of 1 m/s.
    \item Index finger pointing to the left: same as above but the drone rolls towards its left side.
    \item Thumb pointing to the right: it is detected when index finger is extended up (to maintain the drone in offboard control) and the thumb is extended pointing towards the right of the screen. This gesture makes the drone pitch forward at a steady speed of 1 m/s.
    \item Thumb pointing to the left: same as the previous gesture, but the drone pitches backward when the thumb point to the left of the screen.
\end{itemize}
\todo[inline]{Polish: make sure explanation matches behavior after any code changes}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/placeholder.png}
  \caption{Gestures detected by the program to control the movement of the drone}
  \label{fig:hand-gestures}
\end{figure}
\todo[inline]{Figure \ref{fig:hand-gestures}: Clean-up screenshots of detected gestures with drawn landmarks over hand and no background}


The program execution is described in figure \ref{fig:hands-loop}.
After all the necessary options have been set, one new thread is started to run the pilot queue loop detailed in \ref{subsec:pilot-module}, which waits for new commands.
On the main thread, the GUI loop runs endlessly until the user quits the program, queuing actions based on gestures calculated from retrieved images and on user input on the keyboard.
\begin{figure}
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/placeholder.png}
  \caption{Execution flow for the running loop in the hand-gesture control solution.}
  \label{fig:hands-loop}
\end{figure}
\todo[inline]{Figure \ref{fig:hands-loop}: hand solution loop diagram (MIRO)}






\section{Final solution: human following}
\label{sec:follow}

The intention behind the development of a UAV control solution that implements tracking and following of humans is to show how the PX4 open-source development platform and its related projects, MAVLINK and MAVSDK, can be used to design complex real-life applications without the need for expensive state-of-the-art proprietary hardware.
The only requirements of the follow application are a PX4-enabled flight controller installed in an aerial vehicle, a companion computer of appropriate dimensions to be able to be mounted on board the vehicle and any camera that can be connected to the companion computer via USB.
During the program execution, the drone can be controlled via an RC controller, an external ground station application or keyboard input directly to the companion computer through, for example, a secure shell using the SSH protocol.

For safety, the follow mechanism only engages when the flight mode on the vehicle is changed to offboard mode e.g. by activating a configured switch in the RC controller and stops automatically if the connection to the computer is lost or any of the available failsafes are triggered, like low battery, loss of RC or GPS signal or vehicle attitude exceeding a predefined pitch and roll value for longer than a specified time.
In this mode, the vehicle will attempt to find a single person in its field of view and follow their movements by changing its yaw and forward velocity to match horizontal movements and distance changes, respectively.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/pose-landmarks.png}
  \caption{Landmarks extracted from detected human figures by the MediaPipe Pose solution}
  \source{Adapted from \citetitle{mp-pose} \cite{mp-pose}}
  \label{fig:pose-landmarks}
\end{figure}

During the program execution and while the offboard mode and the follow mechanism is engaged, the system continuously retrieves images from the offboard camera that are fed into the MediaPipe Pose \cite{mp-pose-paper} computer vision library to extract pose landmarks from it in the form of 2D coordinates.
Figure~\ref{fig:pose-landmarks} shows the available extracted features and their correspondence to the human body.
These coordinates are used to draw a bounding rectangle around the detected person and to validate that the received landmarks match the general pose of a person standing up.
Figure \ref{fig:pose-validation} shows some examples of the validation checks being run on the raw landmarks extracted from an image.
To prevent unwanted movements, the vehicle will always stop and hover every time it becomes impossible to detect a person in the image received or its features do not match the expected geometry.
After a valid bounding box has been defined around the target person, its position with respect to the camera's field of view is sent to a control mechanism composed of two independent PID controllers. The theory behind these controllers is explained in section \ref{subsec:pid-tools}.

\begin{figure}
  \centering
  \includegraphics[width=8cm, keepaspectratio]{img/placeholder.png}
  \caption{Valid and invalid poses detected by the follow solution}
  \label{fig:pose-validation}
\end{figure}
\todo[inline]{Figure \ref{fig:pose-validation}: valid and invalid person detection from simulator camera output}

The first of the PID controllers is in charge of controlling the yaw velocity of the vehicle to respond to horizontal movements in the x direction of the image.
It takes as input the x coordinate of the center point of the bounding box, and its target is the middle point of the screen.
This controller, therefore, will output velocity commands aimed at maintaining the bounding box centered horizontally in its field of view.
The second PID controller controls the forward velocity of the vehicle to respond to distance changes of the person getting closer or farther away from the drone.
It takes as input the height of the calculated bounding box as a percentage of the total height of the field of view and works to keep it within a value that matches the desired distance to maintain between the person and the vehicle by moving forwards when the height is too low and backwards when it is too high.
The exact percentage of the image height that is covered by a person at a given distance depends on the camera used and needs to be obtained empirically for each separate video source.
Figure \ref{fig:follow-input-calcs} shows how these two inputs are extracted from the coordinates of the bounding box detected around the figure.

\begin{figure}
  \centering
  \includegraphics[width=8cm, keepaspectratio]{img/placeholder.png}
  \caption{Calculation of horizontal position and height of figure from the detected bounding box}
  \label{fig:follow-input-calcs}
\end{figure}
\todo[inline]{Figure \ref{fig:follow-input-calcs}: Draw calculations over extracted pose from simulator}


A summary diagram of the structure of the follow solution can be seen in figure \ref{fig:follow-loop}.
After connecting to the pilot module according to the starting options, a loop runs continuously until the user quits the program.
For each execution, a new image is retrieved, pose features are extracted from it, and a bounding box is calculated.
Then, and only if the vehicle is in offboard flight mode, the calculated positions will be fed into the PID controller to get a velocity output to send to the pilot.
Keyboard control is also available to send manual commands to the vehicle directly.

\begin{figure}
  \centering
  \includegraphics[width=8cm, keepaspectratio]{img/placeholder.png}
  \caption{Execution flow for the running loop in the follow control solution}
  \label{fig:follow-loop}
\end{figure}
\todo[inline]{Figure \ref{fig:follow-loop}: follow loop diagram}




\subsection{PID tools}
\label{subsec:pid-tools}

Two additional utilities have been developed for tuning and measuring the performance of the PID controllers.
These tools are used in section \ref{sec:test-1-pid} to empirically set the correct values in the coefficients for the controllers.
A proportional-integral-derivative is a control loop mechanism commonly used in control systems requiring continuously modulated control.
It works by constantly calculating an error value from the difference between the received input on a chosen process variable, which in this case is the detected position of a person in a the frame, and the desired set point for that variable, a position centered in the frame.
From the error value, and output for the controller is calculated according to this equation:

\begin{equation}
    u(t)= K_p e(t) + K_i \int{e(t)dt} + K_d \frac{de(t)}{dt}
    \label{eq:pid}
\end{equation}
\begin{conditions}
u(t)  &   PID control variable \\
K_p   &   proportional gain \\
K_i   &   integral gain \\
K_d   &   derivative gain \\
e(t)  &   error value \\
de    &   change in error value \\
dt    &   change in time
\end{conditions}

The PID controllers used in this project are developed from the freely-available \texttt{simple-pid} library for Python \cite{pid-library}. which provides all the necessary error calculations already implemented.
It is only necessary to provide the coefficients, or tunings, and the set point for the controller and then call it periodically with an updated input value to receive the output.

Since the coefficients for a controller are most simply decided empirically, a small tool has been developed to help with the process.
With it, it is possible to specify a range of potential coefficients and test the response of the system on the images retrieved from AirSim and a simulated flight controller (either SITL or HITL can be used).
For each of the values to be checked, the simulated person to be followed starts in an offset position from the target center, then the controller is engaged with the test values and its detected position input and calculated velocity output a plotted in a graph for analysis.
Finally, the vehicle moves back to the starting position to reset the environment for the next value to be checked.
The tool is run with the following command: \mintinline{bash}{dronecontrol tools tune}, and can be started with the option \texttt{--yaw} or \texttt{--forward} to decide which specific controller should be tuned.
The other one is deactivated during the test to visualize the effect of the examined values more easily.
Each of the coefficients is tested separately by providing fixed values to the other two parameters.

The second tool designed to check the controllers' performance works in a slightly different way from the previous one.
The underlying process is the same: engage the controller in an offset position and wait until the movement has stopped before resetting and running again.
However, the objective of this tool is to show how an already tuned controller reacts to different position inputs for the camera.
The parameter coefficients then remain the same for the entire test and it is the relative position between the vehicle and the followed person that varies.
During the execution, both the yaw and the forward controller are activated simultaneously to obtain results closer to what will be expected during actual flight.
However, the test tool can be run in either yaw or forward mode, then the distances to be tested between the vehicle and the person vary either left to right or closer to farther, respectively.
The objective is to be able to measure how the vehicle will react to increasingly bigger differences to the target position, to ensure that the movement is stable and safety is maintained during the entire flight.
A full execution of this tool for both modes is shown in section \ref{subsec:pid-test-controller}.


\subsection{Safety mechanisms}
\label{subsec:safety}

The Dronecontrol application implements a very experimental vision-based guidance system.
Therefore, to be able to carry out flight tests with real-life conditions it is necessary to make sure that both the software implements sufficient safety mechanisms so that accidents can be prevented.
Some of these guards employed are part of the developed code and others are activated simply from the PX4 safety configuration.

In the first place, the computer vision module prevents accepting as valid input persons that may have been detected wrongly by the computer vision algorithm.
The method to determine this is to only accept detected feature points that have a minimum resemblance to a standing human pose, with a taller than wider bounding box and the features for head, shoulder, hip, knee and ankle always stacked from top to bottom in that order.
If any of this checks are failed, as well as if there is no detection at all in the image from the algorithm, the vehicle will go into Hold flight mode, which discards any possible pending velocity commands and makes the drone hover over its current position.

The second safety mechanism relates to the PID controllers and pilot module, and consists of limiting the maximum possible velocity of the vehicle at any point during flight.
This is done both inside the Dronecontrol application, by setting up output limits on the PID controllers that prevent the guidance system from sending abnormal velocity commands to the flight controller, and from the PX4 autopilot itself as a fallback in case of issues with the custom MAVLink integration or the companion computer as a whole, by setting the \texttt{MPC\_XY\_VEL\_ALL} parameter in the autopilot configuration which limits the overall horizontal velocity of the vehicle.

Other safety configuration options included as part of the PX4 autopilot detect undesired behaviour in the flight conditions and trigger a flight mode change to either Hold (hover) or Return (fly back to takeoff position and land), as documented in \citetitle{px4-docs-safety} \cite{px4-docs-safety}.
Some of the detected failure conditions are:
\begin{enumerate}
    \item Lost connection to companion computer while in Offboard mode.
    \item RC transmitter link lost while in manual modes, which can be extended to trigger in Offboard mode.
    \item Lost GPS position, when the quality of the PX4 position estimate falls below acceptable levels.
    \item Low battery during flight.
    \item Vehicle unexpectedly flips.  (CBRK\_FLIGHTTERM)
\end{enumerate}

The last safety mechanism to mention is not based on automatic detection by the developed software or the autopilot, but on active surveillance of the systems behaviour during flight by an operator in control of the vehicle.
With an RC controller configured with a switch to control flight mode, it is possible to deactivate Offboard mode at any moment, which will disregard all instructions from the companion computer, and assume full control of the vehicle either through a GPS-assisted mode or completely manual.
A secondary switch in the RC controller can be configured as a kill switch for the last-resort option to immediately stop all motor outputs.
