\section{PID controller design}
\label{sec:test-1-pid}

To implement the person-following mechanism effectively, a pair of PID controllers is employed to derive velocity outputs from position data obtained through image detection. These controllers are defined by their control parameters: the proportional ($K_D$), integral ($K_I$) and derivative ($K_D$) gains, as shown in Equation \ref{eq:pid}. To optimize their performance, it is necessary to fine-tune the controllers by selecting appropriate values for these parameters. These values will be determined through empirical experimentation, acknowledging that obtaining theoretical optical values through this method, as discussed in \ref{subsec:pid-tools}, is not feasible. Nevertheless, it remains the most straightforward approach to achieve a satisfactory enough performance from the closed-loop system to validate the follow control solution. For each parameter, several values will be explored with the aim of striking a balance between a more aggressive controller (characterized by larger gains) for faster control response and a more robust controller (with smaller gains).

The procedure will be as follows. First, the sign of the process gain is selected. For the controller to operate as expected, a positive output should result in an increase in the input. If the system works in the opposite way, the feedback will lead to unstable behaviour where the process variable grows exponentially. This behaviour can be fixed by inverting the sign of the output of the controller before it is fed back into the process.

The second step will focus on selecting the control parameters mentioned before. In the initial phase, the controller operates exclusively as a pure P-controller, with both the I-portion and D-portion deactivated, thereby isolating the proportional gain ($K_P$). To find an optimal value for this parameter, different values of $K_P$ are tested systematically. Starting with a relatively low gain to ensure gradual changes in the process variable, the values are progressively increased until a distinct overshoot is observed, which should quickly diminish without noticeable oscillations.

Utilizing a P-only controller inevitably leaves a residual control deviation, preventing the setpoint from being precisely reached. To address this residual error, it becomes necessary to introduce an integral gain that gradually compensates for the error over time. During this phase, the proportional gain remains set at the previously determined value, while the integral gain is gradually increased until the control deviation is adequately mitigated. Once again, an overly aggressive setting should be avoided to prevent undesirable oscillations.

A derivative gain can be incorporated to dampen the initial overshoot in the process variable to further enhance the controller's performance. This is accomplished by following a similar incremental approach as before. A suitable starting point for the derivative gain is typically around one-tenth of the integral gain \cite{pid-tuning}.

%------------------------------------------------

\subsubsection{The testing environment}

The outlined tuning method will be applied separately to the yaw and forward controllers, with flight control enabled exclusively in one direction at a time. For this purpose, the custom tuning tool developed for this project and detailed in Section \ref{subsec:pid-tools} will be employed to expedite the testing of various controller parameter values and their subsequent comparison. This tool allows designating which controller (yaw or forward) is enabled for testing and which parameter values will be subject to iteration. In each test, two of the parameters will be set with fixed values, while the third parameter is set sequentially to different values. For each of the values in the sequence, the new tunings are applied to the controller and its step response is plotted.

To capture the step response of the controller under focus, a deliberate offset is introduced between the vehicle and the person model within the simulation environment. This offset positions them away from the reference position at which the controller's input aligns with its setpoint. In the coordinate system of the simulation environment (AirSim), with the vehicle located at $x=0, y=0$ on the ground plane, the reference position for the person model is $x=600, y=0$. Shifting the person model from that position along either the y-axis or the x-axis will trigger a step response in the yaw or forward controller, respectively. Figure \ref{fig:tune-start-pos} shows the reference position for the controllers.

At the end of this process, the outcomes are visualized through a series of graphs generated by the program. These graphs offer insights into the controller's input and output over time and the actual changes in position and velocity recorded by the autopilot telemetry during the test. The specific telemetry variables displayed vary according to the particular controller under analysis. The graphs associated with the yaw controller focus on the heading and yaw speed, while those related to the forward controller focus on the position along the forward axis and ground speed.

Given the potential noise introduced by the image detection mechanisms, it is often more advantageous to fine-tune the controllers by examining the measured position rather than the controller input. This approach is preferred as the internal autopilot controller aids in smoothing out the resulting curves, making it easier to see trends and oscillations.


%-------------------------------------------------

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/pid/tune-ref-pos.jpg}
  \caption{Reference position for the yaw and forward PID controllers. From left to right, the panels show the DroneVisionControl application window, the AirSim simulator world view and the world location of the human model in the simulator. The distance between the vehicle and the person is 600 units in the x direction and 0 units in the y direction.}
  \label{fig:tune-start-pos}
\end{figure}


\subsection{Yaw controller}

The initial focus of the tuning process is on the yaw controller. As mentioned earlier, the first step entails determining the sign of the process gain. In this case, the process variable, or input to the controller, corresponds to the normalized position of the detected person in the horizontal axis of the camera's field of view. Here, a value of 0 denotes the person's location at the left edge of the field of view, while a value of 1 signifies their position at the right edge. When the controller generates a positive output, it results in a positive yaw velocity, causing the vehicle to rotate in a rightward direction. In response, the person within the camera's field of view shifts to the left, reducing the input to the controller. Given that positive outputs should lead to an increase in inputs, the sign of the output velocity needs to be inverted to prevent the undesired exponential growth.

A starting position must be established following the sign's determination and preceding any parameter testing. This starting position introduces an offset for the target person model relative to the reference position illustrated in Figure \ref{fig:tune-start-pos}, provoking a step response within the controller. For the yaw controller, this offset will be 100 units along the y-axis. Given that the vehicle is positioned at the origin in the simulation environment, the person model should be situated at coordinates $x=500, y=100$ before starting the tuning process. Figure \ref{fig:tune-ref-pos-yaw} visually represents this starting position within the simulation environment.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/pid/tune-ref-pos-yaw.jpg}
  \caption{Starting position of the simulator for tuning the yaw controller. The human model is situated 500 units forward and 100 units to the right of the vehicle model.}
  \label{fig:tune-ref-pos-yaw}
\end{figure}

\subsubsection{Proportional component}

The proportional gain is the first parameter that needs to be tuned. The values chosen to test for $K_D$ range from 25 to 150 in steps of 25. To have a P-only controller during the test, the $K_I$ and $K_D$ components are set to 0. The results of the test are shown in Figure \ref{fig:tune-yaw-prop}.


\begin{figure}[H]
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \scalebox{0.55}{\input{img/pid-2/tune_yaw_1.1.pgf}}
    \end{minipage}
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \scalebox{0.55}{\input{img/pid-2/tune_yaw_1.2.pgf}}
    \end{minipage}
    \\
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \scalebox{0.55}{\input{img/pid-2/tune_yaw_1.3.pgf}}
    \end{minipage}
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \scalebox{0.55}{\input{img/pid-2/tune_yaw_1.4.pgf}}
    \end{minipage}
    \label{fig:tune-yaw-prop}
    \caption{Variation of (a) input position and (b) output velocity for different values of $K_{P}$ and $K_I=0$, $K_D=0$ while the yaw controller is engaged.}
\end{figure}


\subsubsection{Integral component}
\subsubsection{Derivative component}



\subsection{Forward controller}

Once the yaw controller has been tuned, the focus shifts to the forward controller. The tuning process for the forward controller mirrors that of the yaw controller. When examining the process gain in this context, it becomes evident that a positive output generated by the controller induces a positive velocity in the forward direction, bringing the vehicle closer to the target person. The input variable is determined by the height of the detected person within the camera's field of view, normalized to the height of the field of view itself. This value increases as the drone approaches the target. Consequently, a positive output velocity naturally leads to an increasing input to the controller, indicating that the controller gain is already set to the correct sign.

Regarding the start position for the environment to tune the forward controller, in this case, an offset is required in terms of the distance between the person and the vehicle along the x-axis. To achieve this offset, the person model will be positioned at coordinates $x=500, y=0$, while the vehicle remains at the origin within the simulated world. This chosen starting position, which sets the person and the vehicle closer together than at the reference point, will result in an initially negative velocity output as the vehicle moves away from the person. Figure \ref{fig:tune-ref-pos-fwd} illustrates this starting position within the simulator.


\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/pid/tune-ref-pos-fwd.jpg}
  \caption{Starting position of the simulator for tuning the forward controller. The human model is situated 450 units forward and centred from the vehicle position.}\label{fig:tune-ref-pos-fwd}
\end{figure}

\subsubsection{Proportional component}
\subsubsection{Integral component}
\subsubsection{Derivative component}

\subsection{PID tuning validation}
\label{subsec:pid-test-controller}

The final validation of the tuning obtained for the controllers will be performed using the \texttt{test-controller} tool described in Section \ref{subsec:pid-tools}. The goal is to check the step response of the controllers for different starting distances and validate their performance when engaged simultaneously. For the first test, the starting distances will vary along the y-axis, and for the second one, they will vary along the x-axis.

In the first test, the positions will vary along the y-axis, meaning that the figure will move from left to right in the field of view of the vehicle. The y-coordinates to be tested will range from -150 to 150 units in increments of 50, while the x-coordinate of the figure in the simulated world will remain fixed at $x=500$.

The results of the first run are shown in Figure \ref{fig:validate-yaw}. The y-coordinates tested range from -150 to 150 units in increments of 50, while the x-coordinate of the figure in the simulated world remains fixed at $x=500$. These changes in position mean that the figure moves from left to right in the field of view of the vehicle, following a line parallel to the lateral axis of the vehicle. To counteract this movement, both the yaw and forward controllers need to engage to reach the reference position.

The plots in Figure \ref{fig:validate-yaw} depict the changes in the normalized horizontal distance and normalized figure height detected by the person recognition algorithm during the time it takes for the vehicle to reach the target distance from the human figure for each tested start position. The target is considered reached when the error is less than 2\% and the output speed at the controller is less than 10\% of the maximum value. The full execution of the \texttt{test-controller} tool with varying lateral positions can be seen in the video found in the project's \href{https://l-gonz.github.io/tfg-giaa-dronecontrol/videos/test-yaw-controller}{page}\footnote{\url{https://l-gonz.github.io/tfg-giaa-dronecontrol/videos/test-yaw-controller}}.

\begin{figure}[H]
  \centering
  \makebox[\textwidth][c]{
  \includegraphics[width=.52\linewidth]{img/pid/validation_yaw.png}
  \includegraphics[width=.52\linewidth]{img/pid/validation_yaw_2.png}}
  \caption{Changes over time in detected horizontal position and height as input for the controllers with different starting positions in the y-axis.}
  \label{fig:validate-yaw}
\end{figure}

During execution, the yaw controller introduces a negative yaw velocity when the figure is on the left half of the camera's field of view and a positive yaw velocity when the figure is on the right half, aiming to achieve a target horizontal distance of 0.5 (centred in the camera image). On the other hand, the forward controller outputs a negative forward velocity to reduce the detected height of the figure from around 0.44 to the target value of 0.36.

Looking at Figure \ref{fig:validate-yaw}a, it can be observed that most of the time is spent initiating the movement towards the target. Once the vehicle starts moving, there is not much difference between the -50 and the -150 steps in the time it takes to reach the target position, with the former taking around 3 seconds and the latter taking approximately 3.6 seconds. 

In Figure \ref{fig:validate-yaw}b, the trajectories appear quite similar since the starting distance to the target is the same for all the cases. For each run, the controller guides the vehicle to move backwards, ensuring that the figure stays sufficiently far away, resulting in a decrease in the detected height. Additionally, a detection anomaly can be observed in Figure \ref{fig:validate-yaw}b. For the starting position at $y=100$ (yellow line), there is a brief initial period where the detected height is very small due to a detection error in the first frames processed by the computer vision algorithm. However, within half a second, the detection stabilizes, and the controller successfully guides the vehicle to the target position without significantly impacting the time taken or the final position. This demonstrates that the controllers are capable of recovering from detection errors without compromising the vehicle's movement.


\begin{figure}[H]
  \centering
  \includegraphics[width=.7\textwidth, keepaspectratio]{img/pid/validation_fwd.png}
  \caption{Changes over time in detected height as input for the forward controller with different starting positions in the x-axis.}
  \label{fig:validate-fwd}
\end{figure}


To further validate the performance of the forward controller, the \texttt{test-controller} tool can be used with varying positions along the x-axis. This means that the tests are conducted with the human figure at different distances along the longitudinal axis of the vehicle, closer and further away than the reference distance. Throughout the process, the figure is kept centred in the camera's field of view (y position remains 0). Therefore, in this scenario, the yaw controller does not need to be considered.

Figure \ref{fig:validate-fwd} illustrates the changes over time in the input to the forward controller for each starting position as the vehicle moves towards the target position. The graph highlights significant differences in how the controller responds to positions closer or further away from the target distance. When the person is very close to the vehicle, there are substantial differences in detected heights for minor changes in longitudinal distance, leading to the rapid movement of the vehicle away from its start position. Conversely, when the person is further away from the vehicle than the target, the same differences in distance are associated with minor differences in detected height. As a result, the controller determines a smaller velocity output compared to the cases when the person is closer to the vehicle. Consequently, it takes a longer time for the vehicle to reach the target position.

Notably, even when the person is so close to the vehicle that part of their figure falls outside the camera's field of view, the pose detection mechanism functions well enough to estimate the person's continued position outside the image. This can be seen on Figure \ref{fig:validate-fwd} for the case of $x=300$, where the detected height starts lower than it should be and gradually increases as the full person starts to fit in the camera's field of view.
\\ \\


\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/video-follow-sitl.png}
  \caption{Single frame from the video showing the movement of the drone in response to changes in the position of the tracked person.}
  \label{fig:airsim-test-follow}
\end{figure}

Overall, the results of the validation process indicate that the tuned controllers perform effectively, and accurately follow the person across different starting positions, even when detection errors occur. They successfully respond to variations in the person's distance and angle from the vehicle, allowing for accurate tracking and movement towards the target relative position.

The selected coefficients for the controllers can now be applied to the complete follow solution in order to assess the expected performance of the vehicle in real flights. To provide a visual demonstration, a video showcasing the drone's movement using these parameter values can be accessed \href{https://l-gonz.github.io/tfg-giaa-dronecontrol/videos/test-sitl-follow}{here}\footnote{\url{https://l-gonz.github.io/tfg-giaa-dronecontrol/videos/test-sitl-follow}}. Additionally, Figure \ref{fig:airsim-test-follow} displays a frame extracted from the video, giving a glimpse of the drone's behaviour during the follow operation.



