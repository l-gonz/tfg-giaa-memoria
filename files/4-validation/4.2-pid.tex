\section{PID controller design}
\label{sec:test-1-pid}

% Process of tuning the PIDs
% Graphs from test-controller
% Analysis of error

The person-following mechanism relies on two PID controllers to obtain velocity outputs from the positions calculated by the image detection. These controllers need to be tuned to perform their function; that is, appropriate coefficients for the particular system need to be selected for $K_P$, $K_I$ and $K_D$. These coefficients will be chosen experimentally for the project. Even though, as discussed in \ref{subsec:pid-tools}, it is not possible to obtain the theoretical optical values with this method, it is still the simplest way of achieving a good enough behaviour from the closed-loop system to validate the follow control solution. For each of the parameters, several values will be tested. The objective is to find a balance between the more aggressive controller (larger gains) that leads to faster control and the more robust control of smaller gains.

The procedure will be as follows. First, the sign of the process gain is selected. For the controller to work as expected, a positive output results in an increase in the input. If the system works in the opposite way, the feedback will lead to unstable behaviour where the process variable grows exponentially. This behaviour can be fixed by inverting the sign of the output of the controller before it feeds back to the process.

The second step is selecting the control parameters defined in Equation \ref{eq:pid}. Initially, the controller will be operated as a pure P-controller, with the I-portion and the D-portion turned off to select the proportional gain ($K_D)$. To reach a good parameter, different values of $K_P$ are tested, starting with a low enough gain that the process variable changes slowly and gradually increasing until there is a clear overshoot that quickly subsides and before there starts to be a noticeable oscillation.

With a P-only controller, there is always a remaining control deviation so that the setpoint will never be hit exactly. To fix this remaining error, it is necessary to add an integral gain that compensates the error over time. In this step, the proportional gain will be set to the value already selected, and the integral gain will be increased gradually until the control deviation is compensated. As before, a too-aggressive setting will lead to unwanted oscillations.

Finally, to improve the controller further, a derivative gain can be added to dampen the initial overshoot in the process variable, following the same gradually increasing procedure. A good value to start is about a tenth of the integral gain \cite{pid-tuning}. 

%------------------------------------------------

\subsubsection{The testing environment}

The method outlined will be applied to the yaw and forward controllers independently by only enabling flight control in one direction at a time.
The custom tool developed for this project and described in Section \ref{subsec:pid-tools} will be used to facilitate testing many values for the controller parameters and comparing them. This tool allows selecting which controller will be enabled for testing (yaw or forward) and which values will be iterated for the parameters. To tune each gain independently, two of the parameters will be set with one fixed value, and the other will have several values that will be applied sequentially.

To obtain the step response of the controller under focus, the vehicle and the person model in the simulation environment are situated in an offset position from the reference position to which the controller is defined, i.e., the position at which the input at the controller matches its setpoint.
In the coordinate system of the simulation environment (AirSim), with the vehicle located at $x=0, y=0$ on the ground plane, the reference position for the person model is $x=600, y=0$. By situating the person at an offset position along either the y or x-axis, a step response according to the chosen parameters will be triggered on the yaw or forward controller, respectively.
Figure \ref{fig:tune-start-pos} shows the reference position for the controllers.

At the end of the process, the results are visualized in several graphs plotted by the program. The available graphs show the controller input over time and output over time, as well as the actual change in position and velocity measured by the autopilot telemetry over time. The exact telemetry variables depicted depend on the specific controller analysed. The yaw controller graphs focus on the heading in degrees and yaw speed. The forward controller graphs focus on the position along the forward axis in meters and the ground speed.
Due to the noise introduced by the image detection mechanisms, it is often more useful to tune the controllers by looking at the measured position than the controller input, as the internal autopilot controller helps to smooth the resulting curves.


%-------------------------------------------------

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/pid/tune-ref-pos.jpg}
  \caption{Reference position for the yaw and forward PID controllers. From left to right, the panels show the DroneVisionControl application window, the AirSim simulator world view and the world location of the human model in the simulator. The distance between the vehicle and the person is 600 units in the x direction and 0 units in the y direction.}
  \label{fig:tune-start-pos}
\end{figure}


\subsection{Yaw controller}
The first controller to tune will be the yaw controller. As mentioned, the first step is to select the sign of the process gain. In this case, the process variable or input to the controller is the normalized position of the detected person in the horizontal axis of the camera field of view, with 0 being the person situated on the left edge of the field of view and 1 on the right edge. A positive output on the controller produces a positive yaw velocity, which causes the vehicle to rotate towards the right. In response, the person moves to the left in the camera field of view, resulting in a decreasing input to the controller. Since positive outputs should cause increasing inputs, the sign of the output velocity needs to be inverted to avoid exponentially growing behaviour.

Once the sign is selected and before any parameters are tested, the starting position needs to be decided. In the starting position, the target person model is offset from the reference position shown in Figure \ref{fig:tune-start-pos} to provoke a step response in the controller. For the yaw controller, the offset will be 100 units in the y-axis. With the vehicle situated in the origin in the simulation environment, the person model should be situated at $x=500, y=100$. Figure \ref{fig:tune-ref-pos-yaw} shows the starting position in the simulation.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/pid/tune-ref-pos-yaw.jpg}
  \caption{Starting position of the simulator for tuning the yaw controller. The human model is situated 500 units forward and 100 units to the right of the vehicle model.}
  \label{fig:tune-ref-pos-yaw}
\end{figure}

\subsubsection{Proportional component}

The proportional gain is the first parameter that needs to be tuned. The values chosen to test for $K_D$ range from 25 to 150 in steps of 25. To have a P-only controller during the test, the $K_I$ and $K_D$ components are set to 0. The results of the test are shown in Figure \ref{fig:tune-yaw-prop}.


\begin{figure}[H]
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \scalebox{0.55}{\input{img/pid-2/tune_yaw_1.1.pgf}}
    \end{minipage}
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \scalebox{0.55}{\input{img/pid-2/tune_yaw_1.2.pgf}}
    \end{minipage}
    \\
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \scalebox{0.55}{\input{img/pid-2/tune_yaw_1.3.pgf}}
    \end{minipage}
    \begin{minipage}[t]{0.5\linewidth}
        \centering
        \scalebox{0.55}{\input{img/pid-2/tune_yaw_1.4.pgf}}
    \end{minipage}
    \label{fig:tune-yaw-prop}
    \caption{Variation of (a) input position and (b) output velocity for different values of $K_{P}$ and $K_I=0$, $K_D=0$ while the yaw controller is engaged.}
\end{figure}


\subsubsection{Integral component}
\subsubsection{Derivative component}

%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}
%   \centering
%   \makebox[\textwidth][c]{
%   \includegraphics[width=.52\textwidth]{img/pid/yaw/yaw_pos_prop_i0_d0.png}
%   \includegraphics[width=.52\textwidth]{img/pid/yaw/yaw_vel_prop_i0_d0.jpg}}
%   \caption{Variation of (a) input position and (b) output velocity for different values of $K_{P}$ and $K_I=0$, $K_D=0$ while the yaw controller is engaged.}\label{fig:tune-yaw-prop}
% \end{figure}





\subsection{Forward controller}

After the yaw controller is tuned, it is time to move to the forward controller.
The process will be the same as for the yaw controller. Looking at the process gain in this case, it can be noted that a positive output in the controller creates a positive velocity in the forward direction, drawing the vehicle closer to the target person. The input variable is the height of the detected person in the camera field of view, normalized to the height of the field of view, which increases as the drone gets closer to the target. Therefore, a positive output velocity results in an increasing input to the controller, which means that the controller gain already has the correct sign.

The starting position for tuning the forward controller needs to present an offset from the reference position in the distance between the person and the vehicle (x-axis). To achieve this, the person model will be situated at the $x=500, y=0$ coordinates, with the vehicle remaining at the origin in the simulated world. The chosen starting position (closer than the reference point) will result in an initial negative velocity output to move the vehicle away from the person. Figure \ref{fig:tune-ref-pos-fwd} shows the starting position in the simulator.


\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/pid/tune-ref-pos-fwd.jpg}
  \caption{Starting position of the simulator for tuning the forward controller. The human model is situated 450 units forward and centred from the vehicle position.}\label{fig:tune-ref-pos-fwd}
\end{figure}



\subsection{PID tuning validation}
\label{subsec:pid-test-controller}

The final validation of the tuning obtained for the controllers will be performed using the \texttt{test-controller} tool described in Section \ref{subsec:pid-tools}. The goal is to check the step response of the controllers for different starting distances and validate their performance when engaged simultaneously. For the first test, the starting distances will vary along the y-axis, and for the second one, they will vary along the x-axis.

In the first test, the positions will vary along the y-axis, meaning that the figure will move from left to right in the field of view of the vehicle. The y-coordinates to be tested will range from -150 to 150 units in increments of 50, while the x-coordinate of the figure in the simulated world will remain fixed at $x=500$.

The results of the first run are shown in Figure \ref{fig:validate-yaw}. The y-coordinates tested range from -150 to 150 units in increments of 50, while the x-coordinate of the figure in the simulated world remains fixed at $x=500$. These changes in position mean that the figure moves from left to right in the field of view of the vehicle, following a line parallel to the lateral axis of the vehicle. To counteract this movement, both the yaw and forward controllers need to engage to reach the reference position.

The plots in Figure \ref{fig:validate-yaw} depict the changes in the normalized horizontal distance and normalized figure height detected by the person recognition algorithm during the time it takes for the vehicle to reach the target distance from the human figure for each tested start position. The target is considered reached when the error is less than 2\% and the output speed at the controller is less than 10\% of the maximum value. The full execution of the \texttt{test-controller} tool with varying lateral positions can be seen in the video found in the project's \href{https://l-gonz.github.io/tfg-giaa-dronecontrol/videos/test-yaw-controller}{page}\footnote{\url{https://l-gonz.github.io/tfg-giaa-dronecontrol/videos/test-yaw-controller}}.

\begin{figure}[H]
  \centering
  \makebox[\textwidth][c]{
  \includegraphics[width=.52\linewidth]{img/pid/validation_yaw.png}
  \includegraphics[width=.52\linewidth]{img/pid/validation_yaw_2.png}}
  \caption{Changes over time in detected horizontal position and height as input for the controllers with different starting positions in the y-axis.}
  \label{fig:validate-yaw}
\end{figure}

During execution, the yaw controller introduces a negative yaw velocity when the figure is on the left half of the camera's field of view and a positive yaw velocity when the figure is on the right half, aiming to achieve a target horizontal distance of 0.5 (centred in the camera image). On the other hand, the forward controller outputs a negative forward velocity to reduce the detected height of the figure from around 0.44 to the target value of 0.36.

Looking at Figure \ref{fig:validate-yaw}a, it can be observed that most of the time is spent initiating the movement towards the target. Once the vehicle starts moving, there is not much difference between the -50 and the -150 steps in the time it takes to reach the target position, with the former taking around 3 seconds and the latter taking approximately 3.6 seconds. 

In Figure \ref{fig:validate-yaw}b, the trajectories appear quite similar since the starting distance to the target is the same for all the cases. For each run, the controller guides the vehicle to move backwards, ensuring that the figure stays sufficiently far away, resulting in a decrease in the detected height. Additionally, a detection anomaly can be observed in Figure \ref{fig:validate-yaw}b. For the starting position at $y=100$ (yellow line), there is a brief initial period where the detected height is very small due to a detection error in the first frames processed by the computer vision algorithm. However, within half a second, the detection stabilizes, and the controller successfully guides the vehicle to the target position without significantly impacting the time taken or the final position. This demonstrates that the controllers are capable of recovering from detection errors without compromising the vehicle's movement.


\begin{figure}[H]
  \centering
  \includegraphics[width=.7\textwidth, keepaspectratio]{img/pid/validation_fwd.png}
  \caption{Changes over time in detected height as input for the forward controller with different starting positions in the x-axis.}
  \label{fig:validate-fwd}
\end{figure}


To further validate the performance of the forward controller, the \texttt{test-controller} tool can be used with varying positions along the x-axis. This means that the tests are conducted with the human figure at different distances along the longitudinal axis of the vehicle, closer and further away than the reference distance. Throughout the process, the figure is kept centred in the camera's field of view (y position remains 0). Therefore, in this scenario, the yaw controller does not need to be considered.

Figure \ref{fig:validate-fwd} illustrates the changes over time in the input to the forward controller for each starting position as the vehicle moves towards the target position. The graph highlights significant differences in how the controller responds to positions closer or further away from the target distance. When the person is very close to the vehicle, there are substantial differences in detected heights for minor changes in longitudinal distance, leading to the rapid movement of the vehicle away from its start position. Conversely, when the person is further away from the vehicle than the target, the same differences in distance are associated with minor differences in detected height. As a result, the controller determines a smaller velocity output compared to the cases when the person is closer to the vehicle. Consequently, it takes a longer time for the vehicle to reach the target position.

Notably, even when the person is so close to the vehicle that part of their figure falls outside the camera's field of view, the pose detection mechanism functions well enough to estimate the person's continued position outside the image. This can be seen on Figure \ref{fig:validate-fwd} for the case of $x=300$, where the detected height starts lower than it should be and gradually increases as the full person starts to fit in the camera's field of view.
\\ \\


\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/video-follow-sitl.png}
  \caption{Single frame from the video showing the movement of the drone in response to changes in the position of the tracked person.}
  \label{fig:airsim-test-follow}
\end{figure}

Overall, the results of the validation process indicate that the tuned controllers perform effectively, and accurately follow the person across different starting positions, even when detection errors occur. They successfully respond to variations in the person's distance and angle from the vehicle, allowing for accurate tracking and movement towards the target relative position.

The selected coefficients for the controllers can now be applied to the complete follow solution in order to assess the expected performance of the vehicle in real flights. To provide a visual demonstration, a video showcasing the drone's movement using these parameter values can be accessed \href{https://l-gonz.github.io/tfg-giaa-dronecontrol/videos/test-sitl-follow}{here}\footnote{\url{https://l-gonz.github.io/tfg-giaa-dronecontrol/videos/test-sitl-follow}}. Additionally, Figure \ref{fig:airsim-test-follow} displays a frame extracted from the video, giving a glimpse of the drone's behaviour during the follow operation.



