\section{Final solution: human following}
\label{sec:follow}

The intention behind developing a UAV control solution that implements tracking and following of humans is to demonstrate the capabilities of the PX4 open-source development platform and its related projects, MAVLink and MAVSDK. The goal is to showcase how complex real-life applications can be designed without relying on expensive proprietary hardware. The follow application requires only a PX4-enabled flight controller installed in an aerial vehicle, a companion computer of appropriate dimensions mounted on board the vehicle, and a camera connected to the companion computer via USB.

In this solution, the vehicle will attempt to detect a single person in its field of view and follow their movements by changing its yaw and forward velocity to match horizontal movements and distance changes, respectively.
During program execution, the drone can be controlled via an RC controller, an external ground station application or keyboard input directly to the companion computer through, for example, a remote shell (SSH) or a desktop sharing program.

For safety reasons, the follow mechanism only engages when the flight mode on the vehicle is changed to Offboard mode. This can be done by activating a configured switch on the RC controller. The self-guided control automatically stops if the connection to the computer is lost or if any of the configured fail-safes are triggered, such as low battery, loss of RC or GPS signal, or exceeding predefined pitch and roll limits for a specified time.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/pose-landmarks.png}
  \caption{Landmarks extracted from detected human figures by the MediaPipe Pose solution.}
  \source{Adapted from \citetitle{mp-pose} \cite{mp-pose}}
  \label{fig:pose-landmarks}
\end{figure}

In offboard mode with the follow mechanism engaged, the system continuously retrieves images from the onboard camera. These images are processed using the MediaPipe Pose \cite{mp-pose-paper} computer vision library to extract pose landmarks in the form of 2D coordinates. 
Figure \ref{fig:pose-landmarks} shows the features extracted by the algorithm and their correspondence to the human body.
The extracted landmarks are used to draw a bounding rectangle around the detected person and validate that the received landmarks match the expected pose of a person standing up.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/pose-validation.jpg}
  \caption{Valid versus invalid poses detected by the follow solution.}
  \label{fig:pose-validation}
\end{figure}

Figure \ref{fig:pose-validation} shows some examples of the validation checks running on the raw landmarks extracted from an image.
To ensure controlled movements, the vehicle will stop and hover if it becomes impossible to detect a person in the image or if the detected features do not match the expected pose. Once a valid bounding box is defined around the target person, its position in the image is sent to a control mechanism consisting of two independent PID controllers. These control the vehicle's velocity in the yaw and forward direction, respectively. Section \ref{subsec:pid-tools} describes the mechanisms the PID controllers employ to translate detected positions to output velocities and how they are customized for the project.

\todo[inline]{Explain validation better}
\todo[inline]{Explain noise introduced by image detection}



The follow\footnote{\url{https://github.com/l-gonz/tfg-giaa-dronecontrol/blob/main/dronecontrol/follow/follow.py}} module's structure is summarized in a diagram in Figure \ref{fig:follow-loop}.
In each execution, after connecting to the pilot module with the starting options provided, the main loop runs continuously until the user quits the program.
For each iteration, an image is retrieved, pose features are extracted, and a bounding box is calculated. If the vehicle is in Offboard flight mode, the calculated positions are used as inputs for the PID controllers, which generate velocity outputs to be sent to the pilot module. Manual keyboard control is also available to send direct commands to the vehicle.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth, keepaspectratio]{img/follow-loop.jpg}
  \caption{Execution flow for the running loop in the follow control solution.}
  \label{fig:follow-loop}
\end{figure}

A full execution of this solution is shown in flight in Section \ref{subsec:fl-test-5}.


\subsection{PID controllers}
\label{subsec:pid-tools}

A proportional-integral-derivative (PID) is a control loop mechanism commonly used in systems requiring continuously modulated control. One of its biggest advantages is that it relies only on the response of a single measured process variable, not on knowledge or a model of the underlying process. The controller works by continuously calculating an error value from the difference between the received input on a chosen process variable, which in this case is the detected position of a person in the frame, and the desired set point for that variable, a position centred in the frame. From the error value, the output for the controller is calculated according to Equation \ref{eq:pid}. This output is then fed back into the system as a velocity applied to the vehicle. The change in velocity causes a difference in the detected position, which serves as the next input for the controller, creating a closed control loop.

\begin{equation}
    u(t)= K_p e(t) + K_i \int{e(t)dt} + K_d \frac{de(t)}{dt}
    \label{eq:pid}
\end{equation}
\begin{conditions}
u(t)  &   PID control variable \\
K_p   &   proportional gain \\
K_i   &   integral gain \\
K_d   &   derivative gain \\
e(t)  &   error value \\
de    &   change in error value \\
dt    &   change in time
\end{conditions}

The error value is calculated as the difference between the setpoint $r(t)$ and the process variable or input to the controller, $y(t)$:
\begin{equation}
    e(t)= r(t) - y(t)
\end{equation}

The output obtained from the controller is composed of three individual components. The proportional component, characterized by the proportional gain $K_P$, acts proportionally to the current value of the error. On a P-only controller, there is often a remaining control deviation after an equilibrium is reached which causes the set point not to be reached exactly. The integral component, characterized by the integral gain $K_I$, accounts for past error values, accumulating them over time to eliminate the residual error. The derivative component, characterized by the derivative gain $K_D$, attempts to estimate the future trend of the error by reacting to its rate of change, increasing or dampening the control as the error accelerates or decelerates. Each mentioned gain must be particularized to obtain an effective PID controller for the system under study.

While a PID controller is defined by three control terms, certain scenarios adapt better to using only one or two of these terms to achieve precise control. In such instances, the unused parameters are effectively set to zero, resulting in what is commonly referred to as a PI, PD, P, or I controller, depending on the specific control actions involved. PI controllers are often employed when the derivative action might be susceptible to measurement noise. Nonetheless, the integral term remains crucial in many instances, as it is pivotal in guiding the system towards its intended target value. The process of deciding the control parameters, or tuning, for this particular application is described in Section \ref{sec:test-1-pid}.

As previously mentioned, DroneVisionControl implements two separate PID controllers to control the vehicle's velocity.
The first one is responsible for controlling the yaw velocity of the vehicle, responding to movement in the horizontal axis of the camera's field of view to attempt to keep the person centred horizontally in the field of view. To achieve this, the process variable or input fed to the controller is x-coordinate of the bounding box position in the camera image, normalized to the width of the image, and the setpoint is the middle of the screen (0.5).
The second PID controller directs the forward velocity of the vehicle to respond to changes in the distance between the followed person and the drone. It adjusts the drone's forward movement to keep the height of the bounding box (as a percentage of the total image height) within a desired range. Since the perceived height in the image depends on the camera perspective, the setpoint for this controller should be determined experimentally for each video source.
In the AirSim simulator, a setpoint of 0.5 maintains the vehicle and the person separated approximately 4 meters. The setpoint for the physical camera used for flight test is determined by analysing images taken during flight as described in Section \ref{sec:test-2-sitl}.
\todo[inline]{Check setpoints after pid work}

Figure \ref{fig:follow-input-calcs} shows how the inputs for each controller are extracted from the coordinates of the bounding box detected around the figure.
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth, keepaspectratio]{img/pose-calculations.jpg}
  \caption{Calculation of controller inputs from bounding box drawn around the detected figure.}
  \label{fig:follow-input-calcs}
\end{figure}

The velocities that the controllers output are sent to the pilot module to be communicated to the autopilot through MAVLink messages. However, these velocities do not exactly match the speeds measured by the vehicle's telemetry. The autopilot includes its own internal controllers that translate received commands into signals for the propeller motors to run at a specific speed and maintain a smooth flight. This additional step between the output of the controllers in DroneVisionControl and the vehicle's actual speed will have to be taken into account when the control parameters are determined. In practice, it will result in the possibility of implementing more aggressive behaviour on the controllers, as the more complex internal autopilot controllers will end up smoothing the final trajectory.

The PID controllers in this project are implemented with the help of the open source \texttt{simple-pid} Python library \cite{pid-library}, which supplies all the necessary calculations.
It is only necessary to provide the coefficients or tunings ($K_P$, $K_I$, $K_D$) and the set point (target value) for the controller at the start and then update it periodically with the current input value to receive the output velocity.
In the DroneVisionControl application, it is the internal \texttt{controller}\footnote{\url{https://github.com/l-gonz/tfg-giaa-dronecontrol/blob/main/dronecontrol/follow/controller.py}} module the one that interacts with the external \texttt{simple-pid} library to feed and receive the correct values to the PID controllers calculated from the bounding box around the detected person.

\subsubsection{PID tools}
An additional utility called \texttt{tune\_controller}\footnote{\url{https://github.com/l-gonz/tfg-giaa-dronecontrol/blob/main/dronecontrol/tools/tune_controller.py}} have been developed for tuning and testing the response of the PID controllers. Tuning a PID controller typically involves testing different combinations of coefficients empirically. The \texttt{tune\_controller} tool facilitates this process by allowing users to specify a range of potential coefficients and testing the system's response using images retrieved from AirSim and a simulated flight controller (SITL or HITL). The tool sets up a simulated person in an offset position from the target centre, engages the controller with the test values, and plots the detected position input and calculated velocity output on a graph for analysis. After each test, the vehicle returns to the starting position to reset the environment for the next coefficient to be checked. The tool can be executed using the command \texttt{dronevisioncontrol tools tune} and can be started with the option \texttt{--yaw} or \texttt{--forward} to tune a specific controller while deactivating the other one for better visualization. Each coefficient can be tested individually by providing fixed values for the other two parameters.
\todo[inline]{Fix end}


\subsection{Safety mechanisms}
\label{subsec:safety}

The DroneVisionControl application implements a very experimental vision-based guidance system.
Therefore, to carry out flight tests in real-life conditions, it is necessary to ensure that there are sufficient safety mechanisms to prevent accidents.
These measures include both software-based defences within the application code and safety configurations provided by the PX4 autopilot.

To begin with, the computer vision module in DroneVisionControl verifies the validity of objects detected as a person by examining specific features associated with a standing human figure. It checks that the height of the bounding box is greater than its width and that the features of the head, shoulder, hip, knee, and ankle are detected in the correct order from top to bottom. If any of these checks fail or if no detection output is received, the vehicle immediately enters Hold flight mode. In this mode, any queued velocity commands are discarded, and the drone hovers in its current position. These validation checks are implemented in the \texttt{image\_processing}\footnote{\url{https://github.com/l-gonz/tfg-giaa-dronecontrol/blob/main/dronecontrol/follow/image_processing.py}} module.

In addition to input validation, the application incorporates safety measures related to the PID controllers and the pilot module, which limit the maximum possible velocity of the vehicle at all times during the execution. The PID controllers have output limits set within DroneVisionControl to prevent abnormal velocity commands from being sent to the flight controller. Furthermore, the PX4 autopilot provides a fallback safety measure through its \texttt{MPC\_XY\_VEL\_ALL} parameter, which limits the overall horizontal velocity of the vehicle. These limitations ensure that the drone's movements are within a safe and controllable range.

The PX4 autopilot itself offers various safety configuration options, known as failsafes, documented in \citetitle{px4-docs-safety} \cite{px4-docs-safety}. These failsafes detect undesired conditions during flight and include detecting a lost connection to the companion computer while in Offboard mode, a loss of RC transmitter link or GPS position, low battery levels during flight, and unexpected flipping of the vehicle. When any of these conditions are detected, the autopilot triggers a flight mode change to either Hold (hover) or Return (fly back to takeoff position and land), ensuring the safety of the drone and its surroundings.

The last safety mechanism to mention is not based on automatic detection by the developed software or the autopilot but on active surveillance of the system's behaviour during flight by the operator in control of the vehicle.
With an RC controller configured with a switch to control flight mode, it is possible to deactivate Offboard mode at any moment, which will make the vehicle disregard all instructions from the companion computer and assume complete control of the vehicle either through a GPS-assisted mode or fully manual flight.
A secondary switch in the RC controller can be configured as a kill switch for a last-resort option to stop all motor outputs immediately.
This possibility is most useful when the vehicle is on the ground and cannot manage to take off upwards since there is a danger of breaking the propellers against the ground.

Lastly, operator control plays a vital role in ensuring safety during flight. The operator can use an RC controller with a designated switch to deactivate Offboard mode at any time. This action causes the vehicle to disregard instructions from the companion computer and assume control through GPS-assisted or manual flight modes. Additionally, a secondary switch can be configured as a kill switch, allowing for an immediate stop of all motor outputs when needed. This feature is particularly useful in situations where the vehicle is on the ground and has trouble taking off to reduce the risk of propeller damage if the vehicle flips.

By integrating these safety mechanisms, the DroneVisionControl application aims to mitigate potential risks and ensure safe flight operations under real-life conditions.